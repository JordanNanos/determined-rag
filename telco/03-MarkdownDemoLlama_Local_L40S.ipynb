{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05",
      "metadata": {
        "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05"
      },
      "source": [
        "# Local Llama2 + VectorStoreIndex\n",
        "\n",
        "This notebook walks through the proper setup to use llama-2 with LlamaIndex locally. Note that you need a decent GPU to run this notebook, ideally an A100 with at least 40GB of memory.\n",
        "\n",
        "Specifically, we look at using a vector store index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "adapted from: https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama-Local.html#local-llama2-vectorstoreindex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f09a23",
      "metadata": {
        "id": "91f09a23"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73e14011",
      "metadata": {
        "id": "73e14011",
        "outputId": "ca4a1d2b-7134-40c9-d2a3-435d972e9f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Using cached llama_index-0.9.12-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hCollecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index)\n",
            "  Using cached SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index)\n",
            "  Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Using cached aiostream-0.5.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from llama-index)\n",
            "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "Collecting dataclasses-json (from llama-index)\n",
            "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index)\n",
            "  Using cached fsspec-2023.12.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx (from llama-index)\n",
            "  Using cached httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from llama-index) (1.5.8)\n",
            "Collecting nltk<4.0.0,>=3.8.1 (from llama-index)\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting numpy (from llama-index)\n",
            "  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting openai>=1.1.0 (from llama-index)\n",
            "  Using cached openai-1.3.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pandas (from llama-index)\n",
            "  Using cached pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting requests>=2.31.0 (from llama-index)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index)\n",
            "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Using cached tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from llama-index) (4.8.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipywidgets) (8.18.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipywidgets) (5.14.0)\n",
            "Collecting widgetsnbextension~=4.0.9 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sympy (from torch)\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.1.0 (from torch)\n",
            "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Using cached tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Using cached safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: psutil in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index)\n",
            "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index)\n",
            "  Using cached yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index)\n",
            "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.6->llama-index)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.2->llama-index)\n",
            "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.9.3->llama-index)\n",
            "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: decorator in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: exceptiongroup in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index)\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting anyio<4,>=3.5.0 (from openai>=1.1.0->llama-index)\n",
            "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index)\n",
            "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai>=1.1.0->llama-index)\n",
            "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
            "Collecting sniffio (from openai>=1.1.0->llama-index)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting certifi (from httpx->llama-index)\n",
            "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx->llama-index)\n",
            "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting idna (from httpx->llama-index)\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index)\n",
            "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index)\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index)\n",
            "  Using cached greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n",
            "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from pandas->llama-index) (2.8.2)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index)\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas->llama-index)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting mpmath>=0.19 (from sympy->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index)\n",
            "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index)\n",
            "  Using cached pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Using cached llama_index-0.9.12-py3-none-any.whl (927 kB)\n",
            "Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "Using cached accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
            "Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "Using cached httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Using cached tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Using cached tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "Using cached pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "Using cached greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
            "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "Using cached pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "Using cached yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pytz, mpmath, bitsandbytes, wrapt, widgetsnbextension, urllib3, tzdata, tqdm, tenacity, sympy, soupsieve, sniffio, safetensors, regex, pyyaml, pydantic-core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mypy-extensions, multidict, marshmallow, MarkupSafe, jupyterlab-widgets, joblib, idna, h11, greenlet, fsspec, frozenlist, filelock, distro, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiostream, yarl, typing-inspect, triton, SQLAlchemy, scipy, requests, pydantic, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, jinja2, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, nvidia-cusolver-cu12, huggingface-hub, httpx, dataclasses-json, aiohttp, torch, tokenizers, openai, ipywidgets, transformers, llama-index, accelerate\n",
            "Successfully installed MarkupSafe-2.1.3 SQLAlchemy-2.0.23 accelerate-0.25.0 aiohttp-3.9.1 aiosignal-1.3.1 aiostream-0.5.2 annotated-types-0.6.0 anyio-3.7.1 async-timeout-4.0.3 attrs-23.1.0 beautifulsoup4-4.12.2 bitsandbytes-0.41.2.post2 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.3 deprecated-1.2.14 distro-1.8.0 filelock-3.13.1 frozenlist-1.4.0 fsspec-2023.12.1 greenlet-3.0.1 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface-hub-0.19.4 idna-3.6 ipywidgets-8.1.1 jinja2-3.1.2 joblib-1.3.2 jupyterlab-widgets-3.0.9 llama-index-0.9.12 marshmallow-3.20.1 mpmath-1.3.0 multidict-6.0.4 mypy-extensions-1.0.0 networkx-3.2.1 nltk-3.8.1 numpy-1.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 openai-1.3.7 pandas-2.1.3 pydantic-2.5.2 pydantic-core-2.14.5 pytz-2023.3.post1 pyyaml-6.0.1 regex-2023.10.3 requests-2.31.0 safetensors-0.4.1 scipy-1.11.4 sniffio-1.3.0 soupsieve-2.5 sympy-1.12 tenacity-8.2.3 tiktoken-0.5.2 tokenizers-0.15.0 torch-2.1.1 tqdm-4.66.1 transformers-4.35.2 triton-2.1.0 typing-inspect-0.9.0 tzdata-2023.3 urllib3-2.1.0 widgetsnbextension-4.0.9 wrapt-1.16.0 yarl-1.9.3\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index==0.9.12 ipywidgets==8.1.1 torch==2.1 transformers==4.35.2 accelerate==0.25.0 bitsandbytes==0.41.2.post2 scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119",
      "metadata": {
        "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119"
      },
      "source": [
        "### Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9073233e",
      "metadata": {
        "id": "9073233e"
      },
      "source": [
        "**IMPORTANT**: If you want to use meta-llama models, please sign in to HF hub with an account that has access to the llama2 models, using `huggingface-cli login` in your console. For more details, please see: https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\n",
        "\n",
        "Otherwise, continue with `NOUS` from https://huggingface.co/NousResearch/Llama-2-7b-chat-hf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
      "metadata": {
        "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "be92665d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d5decc100103410f918ffad5786b8c94"
          ]
        },
        "id": "be92665d",
        "outputId": "9b0405ef-6337-427e-9d73-94371915b77d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2155034fce69465e9a2e4fb899e259d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3641dae9936840f4b34162a441088555",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5e992b30b774636bbcbed3140fa3355",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a5744be17a0444e86d202f60398863b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d5b3110aff54296922636817b066e6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6917ccf73c7443a98e3d6254277c0ab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "426c161bb3e44c8280c0fd339ac4af5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a864dd7cc3704a539aa5f0dfe7cc72f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aeaf84d315474864ade7f5584dea4c57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d6dfd37beb246cc821f5febef292b65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a16bb44e94fb449d8db32ace62f3095a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee3aa084b89d4f8782888538abc2f8a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import accelerate\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts import PromptTemplate\n",
        "\n",
        "# Model names (make sure you have access on HF)\n",
        "LLAMA2_7B = \"meta-llama/Llama-2-7b-hf\"\n",
        "LLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "LLAMA2_13B = \"meta-llama/Llama-2-13b-hf\"\n",
        "LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "LLAMA2_70B = \"meta-llama/Llama-2-70b-hf\"\n",
        "LLAMA2_70B_CHAT = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "NOUS = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "selected_model = NOUS\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
        "- Generate human readable output, avoid creating output with gibberish text.\n",
        "- Generate only the requested output, don't include any other language before or after the requested output.\n",
        "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
        "- Generate professional language typically used in business documents in North America.\n",
        "- Never generate offensive or foul language.\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = PromptTemplate(\n",
        "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=2048,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=selected_model,\n",
        "    model_name=selected_model,\n",
        "    device_map=\"cuda:0\",\n",
        "    # change these settings below depending on your GPU\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"load_in_8bit\": False},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390490b1",
      "metadata": {
        "id": "390490b1"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'determined'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 173633, done.\u001b[K\n",
            "remote: Counting objects: 100% (6046/6046), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2460/2460), done.\u001b[K\n",
            "remote: Total 173633 (delta 4062), reused 5011 (delta 3400), pack-reused 167587\u001b[K\n",
            "Receiving objects: 100% (173633/173633), 221.37 MiB | 13.83 MiB/s, done.\n",
            "Resolving deltas: 100% (134222/134222), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/determined-ai/determined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 42 docs of .md format in that github repo\n"
          ]
        }
      ],
      "source": [
        "from llama_index import download_loader\n",
        "from glob import glob\n",
        "\n",
        "MarkdownReader = download_loader(\"MarkdownReader\")\n",
        "markdownreader = MarkdownReader()\n",
        "\n",
        "docs = []\n",
        "for file in glob(\"./determined/*.md\", recursive=True):\n",
        "    docs.extend(markdownreader.load_data(file=file))\n",
        "print(\"Found \" + str(len(docs)) + \" docs of .md format in that github repo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58",
      "metadata": {
        "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af4a79e187ab4950b0fedfc8ef6af7e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2da3d844ede3406ba16a7d3536adaaed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d147e7233bfc478e9650843471c7cb28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad96695bf8a6446598da0a590a39b743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71621044cd5e4d0ea658621a42d0f57c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d1251cec784f1f8920789e508dae51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        "    set_global_service_context,\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=\"local:BAAI/bge-small-en\"\n",
        ")\n",
        "set_global_service_context(service_context)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4",
      "metadata": {
        "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4"
      },
      "source": [
        "## Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
      "metadata": {
        "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
      "metadata": {
        "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
        "outputId": "ab4d4c5f-2b2e-498d-ec30-0fdf51e713c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/home/l40s/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b>The main components of Determined are the Python library, the command line interface (CLI), and the Web UI.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What are the main components of Determined?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Great, I'm happy to help! Based on the context information provided, you can use the CLI to:\n",
              "\n",
              "1. Start a Determined cluster locally using the `det deploy local cluster-up` command.\n",
              "2. Launch Determined on cloud services such as Amazon Web Services (AWS) or Google Cloud Platform (GCP) using the `det deploy aws up` command.\n",
              "3. Train your models using the `det experiment create gpt.yaml .` command.\n",
              "4. Configure distributed training and hyperparameter tuning using YAML files.\n",
              "\n",
              "You can access the WebUI at `http://localhost:8080` after following either set of instructions above. Additionally, you can use the `det` command-line tool to interact with Determined, such as listing available GPUs or CPUs using the `det slot list` command. For more information, please refer to the reference documentation.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What can you use the CLI to do?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Great, let's get started! To install Determined on Kubernetes, you can follow these steps:\n",
              "\n",
              "1. First, make sure you have a Kubernetes cluster set up and running. You can use any Kubernetes distribution, such as Google Kubernetes Engine (GKE), Amazon Elastic Container Service for Kubernetes (EKS), or a self-managed Kubernetes cluster.\n",
              "2. Next, deploy the Determined cluster on your Kubernetes cluster. You can do this by running the following command:\n",
              "```\n",
              "det deploy --kubernetes\n",
              "```\n",
              "This command will deploy the Determined cluster on your Kubernetes cluster using the Kubernetes manifests provided in the `determined/kubernetes` directory.\n",
              "3. Once the deployment is complete, you can access the Determined web interface by navigating to the IP address of your Kubernetes node or by using the Kubernetes service name. For example, if your Kubernetes node is running on `node-1.example.com`, you can access Determined by visiting `http://node-1.example.com:8080`.\n",
              "4. That's it! You should now have Determined installed and running on your Kubernetes cluster. If you have any questions or encounter any issues during the installation process, feel free to reach out to the Determined community for help.\n",
              "\n",
              "I hope this helps! Let me know if you have any other questions.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"How do you install Determined on Kubernetes?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Determined documents are generated using a combination of natural language processing (NLP) and machine learning (ML) techniques. The process involves several steps:\n",
              "\n",
              "1. Content Creation: Determined's content is created by a team of experienced writers and editors who specialize in various industries and topics.\n",
              "2. Structuring: Once the content is created, it is structured using a standardized framework that includes headings, subheadings, and other elements to make it easy to navigate and read.\n",
              "3. Optimization: The structured content is then optimized for search engines using various techniques, such as keyword research, meta tags, and other SEO best practices.\n",
              "4. Publishing: The optimized content is then published to Determined's documentation platform, which is powered by a combination of NLP and ML algorithms.\n",
              "5. Maintenance: Finally, Determined's documentation is maintained through a continuous process of updating, editing, and refining the content to ensure that it remains accurate and relevant.\n",
              "\n",
              "By following these steps, Determined is able to generate high-quality, informative, and easy-to-use documentation that meets the needs of its users.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"How are Determined docs generated?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Of course! If you're looking to write documentation for Determined, there are several resources available to help you get started.\n",
              "\n",
              "Firstly, the Determined community on Slack is an excellent place to ask questions and get support. You can join the community by clicking on the link provided in the context information.\n",
              "\n",
              "Additionally, Determined has a YouTube channel and Twitter account where you can find updates and announcements related to the project.\n",
              "\n",
              "If you prefer to receive updates via email, you can join the community mailing list by visiting the Determined website and filling out the form provided.\n",
              "\n",
              "If you encounter a bug or security issue while working with Determined, you can report it on GitHub or email the security team directly, respectively.\n",
              "\n",
              "Overall, there are several ways to get involved and stay up-to-date with the latest news and developments in the Determined community.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"Tell me about writing docs for Determined\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Determined utilizes Algolia Search as its primary search engine.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What does Determined use Algolia Search for?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Of course! Here's the answer to your query:\n",
              "\n",
              "The Sphinx-Native Search Fallback is a feature in Determined that allows for native search functionality in the absence of Sphinx. This fallback is useful in situations where Sphinx is not available or is not functioning properly, such as during deployment on a non-Sphinx-compatible environment.\n",
              "\n",
              "To enable the Sphinx-Native Search Fallback, you can set the `native_search_fallback` configuration option to `True` in your Determined configuration file. This will enable the fallback and allow Determined to use native search functionality instead of relying on Sphinx.\n",
              "\n",
              "It's important to note that the Sphinx-Native Search Fallback may not provide the same level of performance as Sphinx, as it relies on the underlying file system and may not be able to leverage the full power of Sphinx's indexing capabilities. However, it can still provide a useful fallback in situations where Sphinx is not available or is not functioning properly.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"Tell me about the Sphinx-Native Search Fallback\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24935a47",
      "metadata": {
        "id": "24935a47"
      },
      "source": [
        "### Streaming Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "446406f9",
      "metadata": {
        "id": "446406f9",
        "outputId": "3c2027f0-05f2-4ccb-be0d-99b1caa63007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Great, I'm here to help! A Determined user may choose to use a notebook instead of an experiment for several reasons:\n",
            "\n",
            "1. Flexibility: Notebooks offer more flexibility in terms of data management and analysis. With a notebook, users can easily switch between different datasets, perform ad-hoc analysis, and experiment with different models and algorithms.\n",
            "2. Ease of use: Notebooks are often more user-friendly and easier to use than experiments, especially for users who are new to machine learning. Notebooks provide an interactive environment where users can write code, run experiments, and visualize results without having to worry about setting up and managing complex infrastructure.\n",
            "3. Collaboration: Notebooks are ideal for collaborative work. Multiple users can work on the same notebook simultaneously, and changes are tracked and versioned automatically. This makes it easier to work with teams and share results with others.\n",
            "4. Rapid prototyping: Notebooks allow users to quickly prototype and test ideas without having to set up and manage complex experiments. This can save time and resources, especially in the early stages of a project.\n",
            "\n",
            "Overall, notebooks offer a more flexible, user-friendly, and collaborative way to work with machine learning models, making them a popular choice for many Determined users.</s>\n",
            "\n",
            "Streamed output at 41.84074010934571 tokens/s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"Why would a Determined user choose a notebook vs an experiment?\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, I'd be happy to help!\n",
            "\n",
            "A Determined user may choose an experiment over a notebook for several reasons:\n",
            "\n",
            "1. Reusability: Experiments are designed to be reusable, meaning that the user can easily share their code and results with others. This can be particularly useful for collaborative work or for sharing results with colleagues or supervisors.\n",
            "2. Organization: Experiments are organized in a structured format, making it easier for users to find and reuse code and results. This can help to keep track of progress and to maintain a clear overview of the work being done.\n",
            "3. Version control: Determined provides version control for experiments, which allows users to track changes to their code and results over time. This can be particularly useful for debugging and for maintaining a record of progress.\n",
            "4. Collaboration: Experiments can be easily shared with others, allowing for collaborative work and code reuse. This can be particularly useful for large-scale projects or for working with multiple collaborators.\n",
            "5. Flexibility: Experiments can be customized to fit the user's needs, allowing for greater flexibility in terms of the types of analyses that can be performed. This can be particularly useful for users who need to perform a wide range of analyses or who need to adapt their workflows to changing requirements.\n",
            "\n",
            "Overall, the structured format and reusability of experiments make them a popular choice for Determined users who need to perform complex data analysis tasks.</s>\n",
            "\n",
            "Streamed output at 41.787810498895055 tokens/s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"Why would a Determined user choose an experiment over a notebook?\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'd be happy to help! Based on the context information provided, here's the answer to your query:\n",
            "\n",
            "Notebooks and experiments are both tools used in the Determined platform for training machine learning models. However, they serve different purposes and have different characteristics.\n",
            "\n",
            "A notebook is a single, self-contained document that contains a complete environment for training a model. It includes all the necessary code, data, and configuration settings to train a model from scratch. Notebooks are useful for exploratory work, prototyping, and sharing models with others. They can also be used to document the training process and results.\n",
            "\n",
            "On the other hand, an experiment is a collection of related models that are trained using a shared environment. Experiments are useful for comparing the performance of different models, tuning hyperparameters, and evaluating the effectiveness of different training strategies. Experiments can also be used to reproduce and share the results of previous work.\n",
            "\n",
            "In summary, notebooks are useful for training a single model from scratch, while experiments are useful for comparing and optimizing multiple models in a shared environment.</s>\n",
            "\n",
            "Streamed output at 41.42774729921316 tokens/s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"Please compare the use of notebooks and experiments\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
