{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05",
      "metadata": {
        "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05"
      },
      "source": [
        "# Local Llama2 + VectorStoreIndex\n",
        "\n",
        "This notebook walks through the proper setup to use llama-2 with LlamaIndex locally. Note that you need a decent GPU to run this notebook, ideally an A100 with at least 40GB of memory.\n",
        "\n",
        "Specifically, we look at using a vector store index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "adapted from: https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama-Local.html#local-llama2-vectorstoreindex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f09a23",
      "metadata": {
        "id": "91f09a23"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e14011",
      "metadata": {
        "id": "73e14011",
        "outputId": "ca4a1d2b-7134-40c9-d2a3-435d972e9f11"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index==0.9.12 ipywidgets==8.1.1 torch==2.1 transformers==4.35.2 accelerate==0.25.0 bitsandbytes==0.41.2.post2 scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119",
      "metadata": {
        "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119"
      },
      "source": [
        "### Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9073233e",
      "metadata": {
        "id": "9073233e"
      },
      "source": [
        "**IMPORTANT**: If you want to use meta-llama models, please sign in to HF hub with an account that has access to the llama2 models, using `huggingface-cli login` in your console. For more details, please see: https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\n",
        "\n",
        "Otherwise, continue with `NOUS` from https://huggingface.co/NousResearch/Llama-2-7b-chat-hf "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
      "metadata": {
        "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be92665d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d5decc100103410f918ffad5786b8c94"
          ]
        },
        "id": "be92665d",
        "outputId": "9b0405ef-6337-427e-9d73-94371915b77d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7b75271a59a4bc6b795d6e4df66e2a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import accelerate\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts import PromptTemplate\n",
        "\n",
        "# Model names (make sure you have access on HF)\n",
        "LLAMA2_7B = \"meta-llama/Llama-2-7b-hf\"\n",
        "LLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "LLAMA2_13B = \"meta-llama/Llama-2-13b-hf\"\n",
        "LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "LLAMA2_70B = \"meta-llama/Llama-2-70b-hf\"\n",
        "LLAMA2_70B_CHAT = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "NOUS = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "selected_model = NOUS\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
        "- Generate human readable output, avoid creating output with gibberish text.\n",
        "- Generate only the requested output, don't include any other language before or after the requested output.\n",
        "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
        "- Generate professional language typically used in business documents in North America.\n",
        "- Never generate offensive or foul language.\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = PromptTemplate(\n",
        "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=2048,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=selected_model,\n",
        "    model_name=selected_model,\n",
        "    device_map=\"cuda:0\",\n",
        "    # change these settings below depending on your GPU\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"load_in_8bit\": False},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390490b1",
      "metadata": {
        "id": "390490b1"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'determined' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/determined-ai/determined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 42 docs of .md format in that github repo\n"
          ]
        }
      ],
      "source": [
        "from llama_index import download_loader\n",
        "from glob import glob\n",
        "\n",
        "MarkdownReader = download_loader(\"MarkdownReader\")\n",
        "markdownreader = MarkdownReader()\n",
        "\n",
        "docs = []\n",
        "for file in glob(\"./determined/*.md\", recursive=True):\n",
        "    docs.extend(markdownreader.load_data(file=file))\n",
        "print(\"Found \" + str(len(docs)) + \" docs of .md format in that github repo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58",
      "metadata": {
        "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        "    set_global_service_context,\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=\"local:BAAI/bge-small-en\"\n",
        ")\n",
        "set_global_service_context(service_context)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4",
      "metadata": {
        "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4"
      },
      "source": [
        "## Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
      "metadata": {
        "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
      "metadata": {
        "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
        "outputId": "ab4d4c5f-2b2e-498d-ec30-0fdf51e713c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b>The main components of Determined are the Python library, the command line interface (CLI), and the Web UI.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What are the main components of Determined?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Great, I'm happy to help! Based on the context information provided, you can use the CLI to:\n",
              "\n",
              "1. Start a Determined cluster locally using the `det deploy local cluster-up` command.\n",
              "2. Launch Determined on cloud services such as Amazon Web Services (AWS) or Google Cloud Platform (GCP) using the `det deploy aws up` command.\n",
              "3. Train your models using the `det experiment create gpt.yaml .` command.\n",
              "4. Configure distributed training and hyperparameter tuning using YAML files.\n",
              "\n",
              "You can access the WebUI at `http://localhost:8080` after following either set of instructions above. Additionally, you can use the `det` command-line tool to interact with Determined, such as listing available GPUs or CPUs using the `det slot list` command. For more information, please refer to the reference documentation.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What can you use the CLI to do?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b>Great, let's get started! To install Determined on Kubernetes, you can follow these steps:\n",
              "\n",
              "1. First, you'll need to create a Kubernetes cluster. You can use a cloud provider like AWS or GCP, or you can set up a local cluster using tools like Minikube.\n",
              "2. Once you have a Kubernetes cluster set up, you can deploy Determined to it by using the `det deploy` command. This command will create a Determined cluster on your Kubernetes cluster.\n",
              "3. To deploy Determined on Kubernetes, you'll need to have the `kubectl` command-line tool installed on your system. If you don't have `kubectl`, you can install it using the instructions provided by Kubernetes.\n",
              "4. Once you have `kubectl` installed, you can use the `det deploy` command to deploy Determined to your Kubernetes cluster. For example:\n",
              "```\n",
              "kubectl create deployment determined --image=determined/determined:latest\n",
              "```\n",
              "This command will create a Determined deployment on your Kubernetes cluster using the `determined/determined:latest` image.\n",
              "5. After the deployment is created, you can use the `kubectl get deployment` command to verify that the deployment is running correctly.\n",
              "\n",
              "That's it! With these steps, you should be able to install Determined on your Kubernetes cluster. If you have any questions or need further assistance, feel free to ask.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"How do you install Determined on Kubernetes?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b>Determined documents are generated using a combination of natural language processing (NLP) and machine learning (ML) techniques. The process involves several steps:\n",
              "\n",
              "1. Content Creation: Determined's content is created by a team of experienced writers and editors who specialize in various industries and topics.\n",
              "2. Parsing: The content is then parsed into smaller units such as sentences, phrases, and keywords using NLP techniques.\n",
              "3. Machine Learning: The parsed content is then fed into a machine learning model that analyzes the structure and format of the content.\n",
              "4. Generation: Based on the analysis, the model generates the Determined document using a template that is specific to each topic.\n",
              "5. Review and Editing: The generated document is reviewed and edited by a team of experts to ensure accuracy and consistency.\n",
              "6. Maintenance: Once the document is published, it is continuously updated and maintained to ensure that it remains relevant and up-to-date.\n",
              "\n",
              "By following this process, Determined is able to generate high-quality documents that are informative, well-structured, and easy to read.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"How are Determined docs generated?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b>Of course! If you're looking to write documentation for Determined, there are several resources available to help you get started.\n",
              "\n",
              "Firstly, the Determined community on Slack is an excellent place to ask questions and get support. You can join the community by clicking on the link provided in the context information.\n",
              "\n",
              "Additionally, Determined has a YouTube channel and Twitter account where you can find updates and announcements related to the project.\n",
              "\n",
              "If you prefer to receive updates via email, you can join the community mailing list by visiting the Determined website and filling out the form provided.\n",
              "\n",
              "If you encounter a bug or security issue while working with Determined, you can report it on GitHub or email the security team directly, respectively.\n",
              "\n",
              "Overall, there are several ways to get involved and stay up-to-date with the latest news and developments in the Determined community.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"Tell me about writing docs for Determined\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Determined utilizes Algolia Search as its primary search engine.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What does Determined use Algolia Search for?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>Of course! Here's the answer to your query:\n",
              "\n",
              "The Sphinx-Native Search Fallback is a feature in Determined that allows for native search functionality in the absence of Sphinx. This fallback is useful in situations where Sphinx is not available or is not functioning properly, such as during deployment on a non-Sphinx-compatible environment.\n",
              "\n",
              "To enable the Sphinx-Native Search Fallback, you can set the `native_search_fallback` configuration option to `True` in your Determined configuration file. Once enabled, Determined will use the native search functionality instead of Sphinx for search queries.\n",
              "\n",
              "It's important to note that the native search functionality may not be as powerful or accurate as Sphinx, so it's recommended to use Sphinx whenever possible. However, the Sphinx-Native Search Fallback can be a useful backup option in situations where Sphinx is not available or is not functioning correctly.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"Tell me about the Sphinx-Native Search Fallback\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24935a47",
      "metadata": {
        "id": "24935a47"
      },
      "source": [
        "### Streaming Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "446406f9",
      "metadata": {
        "id": "446406f9",
        "outputId": "3c2027f0-05f2-4ccb-be0d-99b1caa63007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Great, I'm here to help! A Determined user may choose to use a notebook instead of an experiment for several reasons:\n",
            "\n",
            "1. Flexibility: Notebooks offer more flexibility in terms of data management and analysis. With a notebook, users can easily switch between different datasets, perform ad-hoc analysis, and experiment with different models and algorithms.\n",
            "2. Ease of use: Notebooks are often more user-friendly and easier to use than experiments, especially for users who are new to machine learning. Notebooks provide an interactive environment where users can write code, run experiments, and visualize results without having to worry about setting up and managing complex infrastructure.\n",
            "3. Collaboration: Notebooks are ideal for collaborative work. Multiple users can work on the same notebook simultaneously, and changes are tracked and versioned automatically. This makes it easier to work with teams and share results with others.\n",
            "4. Rapid prototyping: Notebooks allow users to quickly prototype and test ideas without having to set up and manage complex experiments. This can save time and resources, especially in the early stages of a project.\n",
            "\n",
            "Overall, notebooks offer a more flexible, user-friendly, and collaborative way to work with machine learning models, which is why a Determined user may choose to use a notebook instead of an experiment.</s>\n",
            "\n",
            "Streamed output at 32.31145797641142 tokens/s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"Why would a Determined user choose a notebook vs an experiment?\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/llama-index/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, I'd be happy to help!\n",
            "\n",
            "A Determined user may choose an experiment over a notebook for several reasons:\n",
            "\n",
            "1. Reusability: Experiments are designed to be reusable, meaning that the user can easily share their code and results with others. This can be particularly useful for collaborative work or for sharing results with colleagues or supervisors.\n",
            "2. Organization: Experiments are organized in a structured format, making it easier for users to find and reuse code and results. This can help to keep track of progress and to maintain a clear overview of the work being done.\n",
            "3. Version control: Determined provides version control for experiments, which allows users to track changes to their code and results over time. This can be particularly useful for debugging and for maintaining a record of progress.\n",
            "4. Collaboration: Experiments can be shared with others, allowing for collaborative work and code reuse. This can be particularly useful for large-scale projects or for projects that require a lot of computational resources.\n",
            "5. Flexibility: Experiments can be customized to meet the specific needs of the user, allowing for greater flexibility in terms of the types of analyses that can be performed.\n",
            "\n",
            "Overall, the choice between an experiment and a notebook will depend on the specific needs and goals of the user. Both options have their own strengths and weaknesses, and the best choice will depend on the context and the nature of the work being done.</s>\n",
            "\n",
            "Streamed output at 32.46786408395942 tokens/s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"Why would a Determined user choose an experiment over a notebook?\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, I'd be happy to help! Based on the context information provided, here's the answer to your query:\n",
            "\n",
            "Notebooks and experiments are both tools used in the Determined platform for training machine learning models. However, they serve different purposes and have different characteristics.\n",
            "\n",
            "A notebook is a single, self-contained document that contains code, markdown, and other content. It can be used to document a model training process, share it with others, or reuse it in the future. Notebooks are useful for exploratory data analysis, prototyping, and sharing models with others.\n",
            "\n",
            "On the other hand, an experiment is a collection of related models, hyperparameters, and training jobs. It allows you to manage and track multiple models and their corresponding hyperparameters, and to run them in parallel on different hardware. Experiments are useful for conducting hyperparameter searches, comparing different models, and scaling up training to multiple GPUs or machines.\n",
            "\n",
            "In summary, notebooks are more focused on documenting and sharing individual model training processes, while experiments are more geared towards managing and scaling up multiple models and hyperparameter searches.</s>\n",
            "\n",
            "Streamed output at 31.6881309203252 tokens/s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"Please compare the use of notebooks and experiments\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
